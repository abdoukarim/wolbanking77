{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43d5c7a4-e1bb-41c6-8209-a8448926e9ba",
   "metadata": {},
   "source": [
    "# Fine-tuning Phi-4 for ASR in Wolof language\n",
    "\n",
    "GPU = A100 PCIe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0220352e-60ee-4a86-b542-8d1095fe3cae",
   "metadata": {},
   "source": [
    "**Acknowledgement:**  \n",
    "This notebook is based on and modified from the original work: [Deep-unlearning/notebooks](https://github.com/Deep-unlearning/notebooks/blob/main/finetune_phi4mm.ipynb).  \n",
    "Thanks to the original author for their contribution.  \n",
    "\n",
    "Let’s get started by installing the necessary libraries.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0f05e4c-fe61-473f-a909-71beb240544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_cache_dir = \"/workspace/hf_cache\"\n",
    "WORKDIR = \"/workspace/Phi4_unf/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ef9915f-55c4-4953-882a-04b213a1e798",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting soundfile\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Downloading numba-0.61.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.24.1)\n",
      "Collecting scipy>=1.6.0 (from librosa)\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn>=1.1.0 (from librosa)\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting joblib>=1.0 (from librosa)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (5.1.1)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-0.5.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.0)\n",
      "Collecting lazy_loader>=0.1 (from librosa)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy_loader>=0.1->librosa) (23.2)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.51.0->librosa)\n",
      "  Downloading llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (3.11.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.31.0)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.1.0->librosa)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2022.12.7)\n",
      "Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.7/260.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.0/378.0 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.61.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading soxr-0.5.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (252 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.44.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, soxr, scipy, msgpack, llvmlite, lazy_loader, joblib, audioread, soundfile, scikit-learn, pooch, numba, librosa\n",
      "Successfully installed audioread-3.0.1 joblib-1.4.2 lazy_loader-0.4 librosa-0.11.0 llvmlite-0.44.0 msgpack-1.1.0 numba-0.61.2 pooch-1.8.2 scikit-learn-1.6.1 scipy-1.15.2 soundfile-0.13.1 soxr-0.5.0.post1 threadpoolctl-3.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29970399-ceb2-423f-bfdb-d3d36a643bb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-17 11:00:17--  https://github.com/Syllo/nvtop/releases/download/3.0.2/nvtop-x86_64.AppImage\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/94530674/a5c730e4-62f8-4ecd-b7c8-814686852756?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250417%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250417T110017Z&X-Amz-Expires=300&X-Amz-Signature=bbf5cbd69af2e0cbe2a4e59dd956b06d72b51bb5033451210cb081c3afd6a0cc&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dnvtop-x86_64.AppImage&response-content-type=application%2Foctet-stream [following]\n",
      "--2025-04-17 11:00:17--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/94530674/a5c730e4-62f8-4ecd-b7c8-814686852756?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250417%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250417T110017Z&X-Amz-Expires=300&X-Amz-Signature=bbf5cbd69af2e0cbe2a4e59dd956b06d72b51bb5033451210cb081c3afd6a0cc&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dnvtop-x86_64.AppImage&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 431296 (421K) [application/octet-stream]\n",
      "Saving to: ‘nvtop-x86_64.AppImage.2’\n",
      "\n",
      "nvtop-x86_64.AppIma 100%[===================>] 421.19K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2025-04-17 11:00:18 (4.25 MB/s) - ‘nvtop-x86_64.AppImage.2’ saved [431296/431296]\n",
      "\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.41.3)\n",
      "Collecting wheel\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (68.2.2)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-78.1.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-78.1.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wheel, setuptools, pip\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.41.3\n",
      "    Uninstalling wheel-0.41.3:\n",
      "      Successfully uninstalled wheel-0.41.3\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 68.2.2\n",
      "    Uninstalling setuptools-68.2.2:\n",
      "      Successfully uninstalled setuptools-68.2.2\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.1\n",
      "    Uninstalling pip-23.3.1:\n",
      "      Successfully uninstalled pip-23.3.1\n",
      "Successfully installed pip-25.0.1 setuptools-78.1.0 wheel-0.45.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.9.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub)\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
      "Collecting tqdm>=4.42.1 (from huggingface_hub)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2022.12.7)\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, fsspec, huggingface_hub\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed fsspec-2025.3.2 huggingface_hub-0.30.2 tqdm-4.67.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.15.2)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.24.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting peft\n",
      "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0+cu118)\n",
      "Collecting transformers (from peft)\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting safetensors (from peft)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.30.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
      "Collecting regex!=2019.12.17 (from transformers->peft)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers->peft)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m238.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, tokenizers, accelerate, transformers, peft\n",
      "Successfully installed accelerate-1.6.0 peft-0.15.2 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting backoff\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: backoff\n",
      "Successfully installed backoff-2.2.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.11.6)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.24.1)\n",
      "Collecting colorama (from sacrebleu)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: tabulate, portalocker, colorama, sacrebleu\n",
      "Successfully installed colorama-0.4.6 portalocker-3.1.1 sacrebleu-2.5.1 tabulate-0.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.24.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.1.0+cu118)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2025.3.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchvision) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchvision) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting hf_transfer\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hf_transfer\n",
      "Successfully installed hf_transfer-0.1.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformers==4.48.2\n",
      "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.2) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.2) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (2022.12.7)\n",
      "Downloading transformers-4.48.2-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.3\n",
      "    Uninstalling transformers-4.51.3:\n",
      "      Successfully uninstalled transformers-4.51.3\n",
      "Successfully installed transformers-4.48.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy_loader>=0.1->librosa) (23.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (3.11.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.24.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading aiohttp-3.11.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m124.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m132.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, requests, pyarrow, propcache, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.16 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.5.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.12.0 multidict-6.4.3 multiprocess-0.70.16 pandas-2.2.3 propcache-0.3.1 pyarrow-19.0.1 pytz-2025.2 requests-2.32.3 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.24.1)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.16)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting jiwer\n",
      "  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting click>=8.1.8 (from jiwer)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
      "  Downloading rapidfuzz-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Downloading jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading rapidfuzz-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, click, jiwer\n",
      "Successfully installed click-8.1.8 jiwer-3.1.0 rapidfuzz-3.13.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/Syllo/nvtop/releases/download/3.0.2/nvtop-x86_64.AppImage\n",
    "!chmod +x nvtop-x86_64.AppImage\n",
    "#!apt install -y htop nano ffmpeg\n",
    "\n",
    "!pip install --upgrade pip wheel setuptools\n",
    "!pip install huggingface_hub\n",
    "!pip install scipy\n",
    "!pip install peft\n",
    "!pip install backoff\n",
    "!pip install accelerate\n",
    "!pip install sacrebleu\n",
    "!pip install torchvision\n",
    "!pip install hf_transfer\n",
    "!pip install transformers==4.48.2\n",
    "!pip install librosa\n",
    "!pip install soundfile\n",
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8fb2baa-ab68-4c46-836e-a584ef009abd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_183/4000422392.py:3: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import get_distribution, DistributionNotFound\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn==2.7.3\n",
      "  Downloading flash_attn-2.7.3.tar.gz (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn==2.7.3) (2.1.0+cu118)\n",
      "Collecting einops (from flash-attn==2.7.3)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.7.3) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.7.3) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.7.3) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.7.3) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.7.3) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.7.3) (2024.12.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.7.3) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn==2.7.3) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn==2.7.3) (1.3.0)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Building wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25l-^C\n",
      "\u001b[?25canceled\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !sudo apt install -y cmake ninja-build\n",
    "\n",
    "from pkg_resources import get_distribution, DistributionNotFound\n",
    "\n",
    "package_name = 'flash_attn'\n",
    "\n",
    "try:\n",
    "  dist = get_distribution(package_name)\n",
    "  print(f\"'{package_name}' version {dist.version} is already installed.\")\n",
    "except DistributionNotFound:\n",
    "  !MAX_JOBS=4 pip install --no-build-isolation flash-attn==2.7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a883f-08ee-455e-b241-1268842c2df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global credential.helper store\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token='HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e45cdf-87e7-48e0-a996-378c95241671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12a936eb3a54dd9ac1cb0a1e157baf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Core Imports\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoProcessor,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BatchFeature,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n",
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model\n",
    ")\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "\n",
    "import jiwer\n",
    "from accelerate.utils import gather_object\n",
    "from datasets import load_dataset, concatenate_datasets, load_from_disk\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "wer_metric = load(\"wer\")\n",
    "\n",
    "\n",
    "INSTRUCTION = \"Transcribe the Wolof audio clip.\"\n",
    "ANSWER_SUFFIX = \"<|end|><|endoftext|>\"\n",
    "_IGNORE_INDEX = -100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e46536b3-9334-4798-a55d-8c935c1716f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/ysdede/trnorm.git\n",
    "#from trnorm.legacy_normalizer import normalize_text as my_custom_normalizer\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Placeholder for text normalization. You can use whisper text normalizer/jiwer or similar tools.\n",
    "    \"\"\"\n",
    "    # return my_custom_normalizer(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b1a8151-bcca-4ebf-a053-b902436885a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WolBanking77Dataset(Dataset):\n",
    "    def __init__(self, processor, dataset, training=True):\n",
    "        \"\"\"\n",
    "        processor: the AutoProcessor instance\n",
    "        dataset: a Hugging Face Dataset (already split into train/validation)\n",
    "        training: whether this dataset is for training (affects concatenation of target tokens)\n",
    "        \"\"\"\n",
    "        self.data = dataset\n",
    "        self.training = training\n",
    "        self.processor = processor\n",
    "        self.instruction = INSTRUCTION\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        # The dataset contains an \"audio\" dict and a \"text\" field for transcription.\n",
    "        user_message = {\n",
    "            'role': 'user',\n",
    "            'content': '<|audio_1|>\\n' + self.instruction,\n",
    "        }\n",
    "        prompt = self.processor.tokenizer.apply_chat_template(\n",
    "            [user_message], tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        inputs = self.processor(\n",
    "            text=prompt,\n",
    "            audios=[(data[\"audio\"][\"array\"], data[\"audio\"][\"sampling_rate\"])],\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        answer = f\"{data['text']}{ANSWER_SUFFIX}\"\n",
    "        answer_ids = self.processor.tokenizer(answer, return_tensors='pt').input_ids\n",
    "        if self.training:\n",
    "            # Concatenate prompt and answer, but mask all tokens except the answer.\n",
    "            input_ids = torch.cat([inputs.input_ids, answer_ids], dim=1)\n",
    "            labels = torch.full_like(input_ids, _IGNORE_INDEX)\n",
    "            labels[:, -answer_ids.shape[1]:] = answer_ids\n",
    "        else:\n",
    "            input_ids = inputs.input_ids\n",
    "            labels = answer_ids\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'input_audio_embeds': inputs.input_audio_embeds,\n",
    "            'audio_embed_sizes': inputs.audio_embed_sizes,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eed228f-29b1-409d-b8cc-6c2e4fbe3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequences, padding_side='right', padding_value=0):\n",
    "    assert padding_side in ['right', 'left']\n",
    "    max_size = sequences[0].size()\n",
    "    trailing_dims = max_size[1:]\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    batch_size = len(sequences)\n",
    "    output = sequences[0].new_full((batch_size, max_len) + trailing_dims, padding_value)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = seq.size(0)\n",
    "        if padding_side == 'right':\n",
    "            output.data[i, :length] = seq\n",
    "        else:\n",
    "            output.data[i, -length:] = seq\n",
    "    return output\n",
    "\n",
    "def cat_with_pad(tensors, dim, padding_value=0):\n",
    "    ndim = tensors[0].dim()\n",
    "    assert all(t.dim() == ndim for t in tensors[1:]), 'All tensors must have the same number of dimensions'\n",
    "    out_size = [max(t.shape[i] for t in tensors) for i in range(ndim)]\n",
    "    out_size[dim] = sum(t.shape[dim] for t in tensors)\n",
    "    output = tensors[0].new_full(out_size, padding_value)\n",
    "    index = 0\n",
    "    for t in tensors:\n",
    "        slices = [slice(0, t.shape[d]) for d in range(ndim)]\n",
    "        slices[dim] = slice(index, index + t.shape[dim])\n",
    "        output[slices] = t\n",
    "        index += t.shape[dim]\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57ab3809-64fb-4b3f-9406-aab83983a53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def esb_collate_fn(batch):\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    input_audio_embeds_list = []\n",
    "    audio_embed_sizes_list = []\n",
    "    audio_attention_mask_list = []\n",
    "    for inputs in batch:\n",
    "        input_ids_list.append(inputs['input_ids'][0])\n",
    "        labels_list.append(inputs['labels'][0])\n",
    "        input_audio_embeds_list.append(inputs['input_audio_embeds'])\n",
    "        audio_embed_sizes_list.append(inputs['audio_embed_sizes'])\n",
    "        audio_attention_mask_list.append(\n",
    "            inputs['input_audio_embeds'].new_full((inputs['input_audio_embeds'].size(1),), True, dtype=torch.bool)\n",
    "        )\n",
    "    try:\n",
    "        input_ids = pad_sequence(input_ids_list, padding_side='left', padding_value=0)\n",
    "        labels = pad_sequence(labels_list, padding_side='left', padding_value=0)\n",
    "        audio_attention_mask = (\n",
    "            pad_sequence(audio_attention_mask_list, padding_side='right', padding_value=False)\n",
    "            if len(audio_attention_mask_list) > 1 else None\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(input_ids_list)\n",
    "        print(labels_list)\n",
    "        raise\n",
    "    attention_mask = (input_ids != 0).long()\n",
    "    input_audio_embeds = cat_with_pad(input_audio_embeds_list, dim=0)\n",
    "    audio_embed_sizes = torch.cat(audio_embed_sizes_list)\n",
    "    return BatchFeature({\n",
    "        'input_ids': input_ids,\n",
    "        'labels': labels,\n",
    "        'attention_mask': attention_mask,\n",
    "        'input_audio_embeds': input_audio_embeds,\n",
    "        'audio_embed_sizes': audio_embed_sizes,\n",
    "        'audio_attention_mask': audio_attention_mask,\n",
    "        'input_mode': 2,  # speech mode\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0f1d0f2-eed1-4839-9acb-290646dfbc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleTokenBatchStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_tokens: torch.LongTensor, batch_size: int = 1) -> None:\n",
    "        self.stop_tokens = stop_tokens\n",
    "        self.max_stop_tokens = stop_tokens.shape[-1]\n",
    "        self.stop_tokens_idx = torch.zeros(batch_size, dtype=torch.long, device=stop_tokens.device)\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        generated_inputs = torch.eq(input_ids[:, -self.max_stop_tokens :].unsqueeze(1), self.stop_tokens)\n",
    "        equal_generated_inputs = torch.all(generated_inputs, dim=2)\n",
    "        sequence_idx = torch.any(equal_generated_inputs, dim=1)\n",
    "        sequence_set_mask = self.stop_tokens_idx == 0\n",
    "        self.stop_tokens_idx[sequence_idx & sequence_set_mask] = input_ids.shape[-1]\n",
    "        return torch.all(self.stop_tokens_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5aecfd1-23a4-4bc3-8f4c-5e3fc66b6cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, processor, eval_dataset, save_path=None, disable_tqdm=False, eval_batch_size=1):\n",
    "    rank = int(os.environ.get('RANK', 0))\n",
    "    local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
    "\n",
    "    model.eval()\n",
    "    all_generated_texts = []\n",
    "    all_labels = []\n",
    "\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=eval_batch_size,\n",
    "        collate_fn=esb_collate_fn,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=8,  # 2 8\n",
    "        prefetch_factor=32,  # 128\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True  # Keep workers alive between batches\n",
    "    )\n",
    "    stop_tokens = [\"<|end|>\", processor.tokenizer.eos_token]\n",
    "    stop_tokens_ids = processor.tokenizer(stop_tokens, add_special_tokens=False, padding=\"longest\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "    stop_tokens_ids = stop_tokens_ids.to(f'cuda:{local_rank}')\n",
    "\n",
    "    # with torch.cuda.amp.autocast(enabled=True):\n",
    "    for inputs in tqdm(eval_dataloader, disable=(rank != 0) or disable_tqdm, desc='running eval'):\n",
    "        stopping_criteria = StoppingCriteriaList([MultipleTokenBatchStoppingCriteria(stop_tokens_ids, batch_size=inputs.input_ids.size(0))])\n",
    "        inputs = inputs.to(f'cuda:{local_rank}')\n",
    "        generated_ids = model.generate(\n",
    "            **inputs, eos_token_id=processor.tokenizer.eos_token_id, max_new_tokens=64,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "        )\n",
    "\n",
    "        stop_tokens_idx = stopping_criteria[0].stop_tokens_idx.reshape(inputs.input_ids.size(0), -1)[:, 0]\n",
    "        stop_tokens_idx = torch.where(\n",
    "            stop_tokens_idx > 0,\n",
    "            stop_tokens_idx - stop_tokens_ids.shape[-1],\n",
    "            generated_ids.shape[-1],\n",
    "        )\n",
    "        generated_text = [\n",
    "            processor.decode(_pred_ids[inputs[\"input_ids\"].shape[1] : _stop_tokens_idx],\n",
    "                              skip_special_tokens=True,\n",
    "                              clean_up_tokenization_spaces=False)\n",
    "            for _pred_ids, _stop_tokens_idx in zip(generated_ids, stop_tokens_idx)\n",
    "        ]\n",
    "\n",
    "        all_generated_texts.extend(generated_text)\n",
    "        labels = [processor.decode(_label_ids[_label_ids != 0]).rstrip(ANSWER_SUFFIX) for _label_ids in inputs[\"labels\"]]  # ⚠ See annd apply: https://huggingface.co/microsoft/Phi-4-multimodal-instruct/discussions/33\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "    all_generated_texts = gather_object(all_generated_texts)\n",
    "    all_labels = gather_object(all_labels)\n",
    "\n",
    "    if rank == 0:\n",
    "        norm_all_labels = normalize_text(all_labels)\n",
    "        norm_all_generated_texts = normalize_text(all_generated_texts)\n",
    "        # wer = jiwer.wer(norm_all_labels, norm_all_generated_texts)\n",
    "        wer = wer_metric.compute(references=norm_all_labels, predictions=norm_all_generated_texts)\n",
    "        print(\"WER:\", wer)\n",
    "        if save_path:\n",
    "            with open(save_path, 'w') as f:\n",
    "                save_dict = {\n",
    "                    'all_generated_texts': all_generated_texts,\n",
    "                    'all_labels': all_labels,\n",
    "                    'wer': wer,\n",
    "                }\n",
    "                json.dump(save_dict, f)\n",
    "        return wer\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ebf3ab-c5a2-48c7-bd8a-1472b102870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"dataset/audio\"\n",
    "\n",
    "ds = load_dataset(\"parquet\", \n",
    "                      data_files={'train': os.path.join(dataset_dir, 'train.parquet'), \n",
    "                                  'test': os.path.join(dataset_dir, 'test.parquet')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd96eb1f-30be-46b9-94c8-0809334e75eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 2563\n",
      "Val dataset size: 641\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE_PER_GPU = 16 # 8\n",
    "EVAL_BATCH_SIZE_PER_GPU = 24\n",
    "\n",
    "# Load and split the dataset.\n",
    "train_ds = ds['train']\n",
    "val_ds = ds['test']\n",
    "\n",
    "num_processes = 8\n",
    "print(f\"Training dataset size: {len(train_ds)}\")\n",
    "print(f\"Val dataset size: {len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7b69c0b-40a9-467e-bf0c-3ee26ae05899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.71.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.24.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard) (23.2)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard)\n",
      "  Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (78.1.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "Downloading grpcio-1.71.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m133.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m132.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Installing collected packages: werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, absl-py, tensorboard\n",
      "Successfully installed absl-py-2.2.2 grpcio-1.71.0 markdown-3.8 protobuf-6.30.2 tensorboard-2.19.0 tensorboard-data-server-0.7.2 werkzeug-3.1.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57a619a8-f931-4fc3-b481-8274cff27eaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1 GPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ccd73946664d309cad2b26e0f4cd35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345f1fadd0fc4876af6dfa73ba831cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing_phi4mm.py:   0%|          | 0.00/32.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-4-multimodal-instruct:\n",
      "- processing_phi4mm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7638f8e3cd4d0b81ba2b95a6360416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9661c9a73b354d61b6f79d43bd0eb652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b3e7b3fddf491daa0a194df5cff9ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/3.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20964d059c4495b8a18d1584561a598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef81369ec7cc495c8cda967a3ccd4a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/15.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f37bb2858f453c956c4cfed93f3392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/249 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dfd33f238fb4212a72f2ee9ad3d286f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431753a0874a4ed88317ceb4b59217cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b765d295e741229c4fcf6788d0ab60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_phi4mm.py:   0%|          | 0.00/11.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-4-multimodal-instruct:\n",
      "- configuration_phi4mm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a74bfa405d446382aab2718025d394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_phi4mm.py:   0%|          | 0.00/116k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657f1820d3334cd89db834f96a8d1e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vision_siglip_navit.py:   0%|          | 0.00/78.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-4-multimodal-instruct:\n",
      "- vision_siglip_navit.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3c310ea15d482b9a4867918411288c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "speech_conformer_encoder.py:   0%|          | 0.00/111k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-4-multimodal-instruct:\n",
      "- speech_conformer_encoder.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-4-multimodal-instruct:\n",
      "- modeling_phi4mm.py\n",
      "- vision_siglip_navit.py\n",
      "- speech_conformer_encoder.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655790f525ce411e835d91e7d4630778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/240k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03dc61bbd1a249a7aa6cdfc3b34677c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b220eb984544fe9af5582a489ec8ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7145ea208f44418c38090c38948c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a8484093f44aa38d6751fbef3524f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fee0aaa9b3444a2a97dbb2b73c3fced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebbb417927ab44318986f75750aee4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 5,574,460,224\n",
      "Unfrozen components:\n",
      "- model.embed_tokens.weight\n",
      "- model.embed_tokens_extend.image_embed.glb_GN\n",
      "- model.embed_tokens_extend.image_embed.sub_GN\n",
      "- model.embed_tokens_extend.image_embed.img_processor.embeddings.patch_embedding.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.embeddings.patch_embedding.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.embeddings.position_embedding.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.0.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.0.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.0.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.0.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.0.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.0.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.0.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.0.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.0.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.0.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.0.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.0.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.0.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.0.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.0.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.0.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.1.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.1.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.1.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.1.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.1.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.1.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.1.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.1.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.1.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.1.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.1.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.1.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.1.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.1.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.1.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.1.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.2.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.2.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.2.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.2.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.2.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.2.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.2.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.2.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.2.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.2.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.2.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.2.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.2.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.2.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.2.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.2.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.3.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.3.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.3.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.3.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.3.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.3.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.3.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.3.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.3.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.3.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.3.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.3.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.3.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.3.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.3.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.3.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.4.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.4.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.4.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.4.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.4.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.4.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.4.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.4.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.4.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.4.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.4.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.4.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.4.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.4.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.4.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.4.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.5.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.5.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.5.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.5.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.5.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.5.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.5.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.5.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.5.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.5.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.5.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.5.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.5.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.5.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.5.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.5.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.6.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.6.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.6.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.6.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.6.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.6.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.6.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.6.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.6.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.6.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.6.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.6.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.6.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.6.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.6.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.6.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.7.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.7.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.7.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.7.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.7.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.7.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.7.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.7.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.7.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.7.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.7.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.7.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.7.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.7.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.7.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.7.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.8.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.8.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.8.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.8.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.8.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.8.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.8.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.8.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.8.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.8.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.8.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.8.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.8.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.8.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.8.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.8.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.9.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.9.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.9.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.9.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.9.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.9.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.9.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.9.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.9.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.9.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.9.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.9.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.9.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.9.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.9.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.9.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.10.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.10.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.10.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.10.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.10.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.10.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.10.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.10.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.10.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.10.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.10.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.10.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.10.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.10.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.10.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.10.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.11.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.11.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.11.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.11.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.11.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.11.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.11.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.11.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.11.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.11.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.11.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.11.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.11.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.11.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.11.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.11.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.12.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.12.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.12.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.12.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.12.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.12.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.12.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.12.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.12.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.12.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.12.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.12.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.12.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.12.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.12.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.12.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.13.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.13.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.13.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.13.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.13.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.13.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.13.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.13.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.13.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.13.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.13.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.13.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.13.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.13.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.13.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.13.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.14.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.14.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.14.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.14.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.14.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.14.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.14.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.14.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.14.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.14.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.14.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.14.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.14.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.14.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.14.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.14.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.15.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.15.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.15.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.15.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.15.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.15.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.15.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.15.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.15.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.15.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.15.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.15.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.15.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.15.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.15.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.15.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.16.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.16.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.16.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.16.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.16.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.16.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.16.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.16.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.16.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.16.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.16.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.16.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.16.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.16.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.16.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.16.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.17.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.17.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.17.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.17.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.17.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.17.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.17.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.17.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.17.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.17.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.17.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.17.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.17.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.17.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.17.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.17.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.18.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.18.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.18.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.18.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.18.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.18.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.18.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.18.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.18.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.18.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.18.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.18.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.18.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.18.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.18.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.18.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.19.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.19.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.19.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.19.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.19.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.19.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.19.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.19.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.19.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.19.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.19.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.19.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.19.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.19.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.19.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.19.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.20.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.20.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.20.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.20.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.20.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.20.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.20.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.20.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.20.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.20.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.20.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.20.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.20.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.20.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.20.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.20.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.21.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.21.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.21.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.21.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.21.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.21.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.21.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.21.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.21.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.21.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.21.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.21.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.21.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.21.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.21.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.21.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.22.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.22.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.22.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.22.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.22.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.22.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.22.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.22.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.22.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.22.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.22.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.22.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.22.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.22.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.22.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.22.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.23.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.23.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.23.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.23.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.23.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.23.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.23.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.23.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.23.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.23.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.23.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.23.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.23.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.23.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.23.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.23.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.24.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.24.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.24.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.24.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.24.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.24.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.24.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.24.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.24.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.24.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.24.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.24.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.24.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.24.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.24.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.24.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.25.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.25.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.25.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.25.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.25.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.25.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.25.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.25.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.25.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.25.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.25.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.25.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.25.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.25.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.25.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.25.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.26.self_attn.k_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.26.self_attn.k_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.26.self_attn.v_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.26.self_attn.v_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.26.self_attn.q_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.26.self_attn.q_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.26.self_attn.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.26.self_attn.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.26.layer_norm1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.26.layer_norm1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.26.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.26.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.26.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.26.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.26.layer_norm2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.encoder.layers.26.layer_norm2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.post_layernorm.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.post_layernorm.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.head.probe\n",
      "- model.embed_tokens_extend.image_embed.img_processor.head.attention.in_proj_weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.head.attention.in_proj_bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.head.attention.out_proj.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.head.attention.out_proj.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.head.layernorm.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.head.layernorm.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.head.mlp.fc1.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.head.mlp.fc1.bias\n",
      "- model.embed_tokens_extend.image_embed.img_processor.head.mlp.fc2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_processor.head.mlp.fc2.bias\n",
      "- model.embed_tokens_extend.image_embed.img_projection.0.weight\n",
      "- model.embed_tokens_extend.image_embed.img_projection.0.bias\n",
      "- model.embed_tokens_extend.image_embed.img_projection.2.weight\n",
      "- model.embed_tokens_extend.image_embed.img_projection.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.embed.out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.embed.out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.embed.conv.0.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.embed.conv.0.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.embed.conv.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.embed.conv.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.embed.conv.3.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.embed.conv.3.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.embed.conv.5.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.embed.conv.5.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.embed.conv.6.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.embed.conv.6.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.relative_attention_bias_layer.bias_values.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.0._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.1._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.2._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.3._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.4._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.5._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.6._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.7._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.8._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.9._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.10._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.11._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.12._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.13._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.14._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.15._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.16._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.17._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.18._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.19._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.20._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.21._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.22._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.feed_forward_in.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.feed_forward_in.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.feed_forward_in.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.feed_forward_in.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.feed_forward_in.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.feed_forward_in.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.self_attn.linear_q.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.self_attn.linear_q.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.self_attn.linear_k.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.self_attn.linear_k.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.self_attn.linear_v.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.self_attn.linear_v.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.self_attn.linear_out.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.self_attn.linear_out.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.conv.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.conv.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.conv.glu.b1\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.conv.glu.b2\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.conv.glu.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.conv.ext_pw_conv_1d.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.conv.ext_pw_conv_1d.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.conv.dw_sep_conv_1d.dw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.conv.dw_sep_conv_1d.pw_conv.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.feed_forward_out.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.feed_forward_out.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.feed_forward_out.net.0.linear.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.feed_forward_out.net.0.linear.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.feed_forward_out.net.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.feed_forward_out.net.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.layer_norm_att.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.layer_norm_att.bias\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.layer_norm.weight\n",
      "- model.embed_tokens_extend.audio_embed.encoder.encoders.23._checkpoint_wrapped_module.layer_norm.bias\n",
      "- model.embed_tokens_extend.audio_embed.audio_projection.speech.0.weight\n",
      "- model.embed_tokens_extend.audio_embed.audio_projection.speech.0.bias\n",
      "- model.embed_tokens_extend.audio_embed.audio_projection.speech.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.audio_projection.speech.2.bias\n",
      "- model.embed_tokens_extend.audio_embed.audio_projection.vision.0.weight\n",
      "- model.embed_tokens_extend.audio_embed.audio_projection.vision.0.bias\n",
      "- model.embed_tokens_extend.audio_embed.audio_projection.vision.2.weight\n",
      "- model.embed_tokens_extend.audio_embed.audio_projection.vision.2.bias\n",
      "- model.layers.0.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.0.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.0.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.0.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.0.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.0.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.0.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.0.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.0.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.0.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.0.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.0.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.0.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.0.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.0.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.0.mlp.down_proj.base_layer.weight\n",
      "- model.layers.0.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.0.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.0.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.0.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.0.input_layernorm.weight\n",
      "- model.layers.0.post_attention_layernorm.weight\n",
      "- model.layers.1.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.1.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.1.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.1.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.1.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.1.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.1.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.1.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.1.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.1.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.1.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.1.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.1.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.1.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.1.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.1.mlp.down_proj.base_layer.weight\n",
      "- model.layers.1.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.1.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.1.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.1.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.1.input_layernorm.weight\n",
      "- model.layers.1.post_attention_layernorm.weight\n",
      "- model.layers.2.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.2.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.2.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.2.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.2.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.2.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.2.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.2.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.2.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.2.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.2.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.2.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.2.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.2.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.2.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.2.mlp.down_proj.base_layer.weight\n",
      "- model.layers.2.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.2.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.2.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.2.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.2.input_layernorm.weight\n",
      "- model.layers.2.post_attention_layernorm.weight\n",
      "- model.layers.3.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.3.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.3.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.3.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.3.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.3.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.3.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.3.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.3.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.3.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.3.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.3.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.3.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.3.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.3.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.3.mlp.down_proj.base_layer.weight\n",
      "- model.layers.3.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.3.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.3.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.3.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.3.input_layernorm.weight\n",
      "- model.layers.3.post_attention_layernorm.weight\n",
      "- model.layers.4.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.4.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.4.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.4.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.4.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.4.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.4.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.4.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.4.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.4.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.4.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.4.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.4.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.4.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.4.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.4.mlp.down_proj.base_layer.weight\n",
      "- model.layers.4.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.4.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.4.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.4.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.4.input_layernorm.weight\n",
      "- model.layers.4.post_attention_layernorm.weight\n",
      "- model.layers.5.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.5.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.5.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.5.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.5.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.5.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.5.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.5.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.5.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.5.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.5.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.5.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.5.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.5.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.5.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.5.mlp.down_proj.base_layer.weight\n",
      "- model.layers.5.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.5.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.5.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.5.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.5.input_layernorm.weight\n",
      "- model.layers.5.post_attention_layernorm.weight\n",
      "- model.layers.6.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.6.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.6.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.6.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.6.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.6.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.6.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.6.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.6.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.6.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.6.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.6.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.6.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.6.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.6.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.6.mlp.down_proj.base_layer.weight\n",
      "- model.layers.6.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.6.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.6.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.6.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.6.input_layernorm.weight\n",
      "- model.layers.6.post_attention_layernorm.weight\n",
      "- model.layers.7.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.7.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.7.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.7.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.7.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.7.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.7.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.7.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.7.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.7.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.7.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.7.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.7.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.7.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.7.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.7.mlp.down_proj.base_layer.weight\n",
      "- model.layers.7.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.7.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.7.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.7.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.7.input_layernorm.weight\n",
      "- model.layers.7.post_attention_layernorm.weight\n",
      "- model.layers.8.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.8.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.8.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.8.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.8.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.8.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.8.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.8.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.8.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.8.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.8.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.8.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.8.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.8.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.8.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.8.mlp.down_proj.base_layer.weight\n",
      "- model.layers.8.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.8.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.8.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.8.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.8.input_layernorm.weight\n",
      "- model.layers.8.post_attention_layernorm.weight\n",
      "- model.layers.9.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.9.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.9.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.9.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.9.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.9.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.9.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.9.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.9.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.9.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.9.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.9.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.9.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.9.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.9.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.9.mlp.down_proj.base_layer.weight\n",
      "- model.layers.9.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.9.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.9.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.9.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.9.input_layernorm.weight\n",
      "- model.layers.9.post_attention_layernorm.weight\n",
      "- model.layers.10.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.10.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.10.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.10.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.10.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.10.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.10.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.10.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.10.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.10.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.10.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.10.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.10.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.10.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.10.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.10.mlp.down_proj.base_layer.weight\n",
      "- model.layers.10.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.10.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.10.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.10.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.10.input_layernorm.weight\n",
      "- model.layers.10.post_attention_layernorm.weight\n",
      "- model.layers.11.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.11.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.11.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.11.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.11.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.11.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.11.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.11.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.11.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.11.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.11.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.11.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.11.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.11.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.11.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.11.mlp.down_proj.base_layer.weight\n",
      "- model.layers.11.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.11.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.11.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.11.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.11.input_layernorm.weight\n",
      "- model.layers.11.post_attention_layernorm.weight\n",
      "- model.layers.12.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.12.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.12.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.12.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.12.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.12.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.12.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.12.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.12.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.12.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.12.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.12.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.12.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.12.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.12.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.12.mlp.down_proj.base_layer.weight\n",
      "- model.layers.12.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.12.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.12.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.12.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.12.input_layernorm.weight\n",
      "- model.layers.12.post_attention_layernorm.weight\n",
      "- model.layers.13.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.13.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.13.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.13.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.13.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.13.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.13.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.13.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.13.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.13.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.13.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.13.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.13.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.13.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.13.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.13.mlp.down_proj.base_layer.weight\n",
      "- model.layers.13.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.13.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.13.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.13.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.13.input_layernorm.weight\n",
      "- model.layers.13.post_attention_layernorm.weight\n",
      "- model.layers.14.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.14.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.14.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.14.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.14.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.14.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.14.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.14.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.14.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.14.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.14.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.14.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.14.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.14.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.14.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.14.mlp.down_proj.base_layer.weight\n",
      "- model.layers.14.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.14.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.14.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.14.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.14.input_layernorm.weight\n",
      "- model.layers.14.post_attention_layernorm.weight\n",
      "- model.layers.15.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.15.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.15.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.15.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.15.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.15.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.15.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.15.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.15.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.15.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.15.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.15.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.15.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.15.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.15.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.15.mlp.down_proj.base_layer.weight\n",
      "- model.layers.15.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.15.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.15.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.15.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.15.input_layernorm.weight\n",
      "- model.layers.15.post_attention_layernorm.weight\n",
      "- model.layers.16.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.16.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.16.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.16.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.16.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.16.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.16.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.16.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.16.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.16.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.16.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.16.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.16.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.16.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.16.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.16.mlp.down_proj.base_layer.weight\n",
      "- model.layers.16.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.16.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.16.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.16.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.16.input_layernorm.weight\n",
      "- model.layers.16.post_attention_layernorm.weight\n",
      "- model.layers.17.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.17.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.17.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.17.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.17.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.17.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.17.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.17.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.17.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.17.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.17.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.17.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.17.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.17.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.17.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.17.mlp.down_proj.base_layer.weight\n",
      "- model.layers.17.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.17.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.17.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.17.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.17.input_layernorm.weight\n",
      "- model.layers.17.post_attention_layernorm.weight\n",
      "- model.layers.18.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.18.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.18.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.18.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.18.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.18.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.18.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.18.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.18.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.18.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.18.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.18.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.18.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.18.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.18.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.18.mlp.down_proj.base_layer.weight\n",
      "- model.layers.18.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.18.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.18.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.18.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.18.input_layernorm.weight\n",
      "- model.layers.18.post_attention_layernorm.weight\n",
      "- model.layers.19.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.19.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.19.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.19.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.19.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.19.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.19.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.19.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.19.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.19.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.19.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.19.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.19.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.19.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.19.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.19.mlp.down_proj.base_layer.weight\n",
      "- model.layers.19.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.19.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.19.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.19.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.19.input_layernorm.weight\n",
      "- model.layers.19.post_attention_layernorm.weight\n",
      "- model.layers.20.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.20.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.20.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.20.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.20.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.20.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.20.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.20.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.20.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.20.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.20.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.20.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.20.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.20.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.20.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.20.mlp.down_proj.base_layer.weight\n",
      "- model.layers.20.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.20.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.20.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.20.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.20.input_layernorm.weight\n",
      "- model.layers.20.post_attention_layernorm.weight\n",
      "- model.layers.21.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.21.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.21.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.21.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.21.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.21.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.21.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.21.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.21.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.21.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.21.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.21.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.21.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.21.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.21.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.21.mlp.down_proj.base_layer.weight\n",
      "- model.layers.21.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.21.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.21.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.21.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.21.input_layernorm.weight\n",
      "- model.layers.21.post_attention_layernorm.weight\n",
      "- model.layers.22.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.22.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.22.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.22.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.22.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.22.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.22.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.22.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.22.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.22.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.22.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.22.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.22.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.22.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.22.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.22.mlp.down_proj.base_layer.weight\n",
      "- model.layers.22.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.22.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.22.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.22.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.22.input_layernorm.weight\n",
      "- model.layers.22.post_attention_layernorm.weight\n",
      "- model.layers.23.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.23.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.23.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.23.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.23.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.23.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.23.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.23.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.23.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.23.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.23.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.23.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.23.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.23.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.23.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.23.mlp.down_proj.base_layer.weight\n",
      "- model.layers.23.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.23.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.23.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.23.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.23.input_layernorm.weight\n",
      "- model.layers.23.post_attention_layernorm.weight\n",
      "- model.layers.24.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.24.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.24.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.24.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.24.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.24.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.24.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.24.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.24.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.24.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.24.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.24.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.24.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.24.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.24.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.24.mlp.down_proj.base_layer.weight\n",
      "- model.layers.24.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.24.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.24.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.24.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.24.input_layernorm.weight\n",
      "- model.layers.24.post_attention_layernorm.weight\n",
      "- model.layers.25.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.25.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.25.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.25.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.25.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.25.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.25.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.25.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.25.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.25.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.25.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.25.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.25.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.25.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.25.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.25.mlp.down_proj.base_layer.weight\n",
      "- model.layers.25.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.25.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.25.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.25.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.25.input_layernorm.weight\n",
      "- model.layers.25.post_attention_layernorm.weight\n",
      "- model.layers.26.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.26.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.26.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.26.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.26.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.26.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.26.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.26.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.26.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.26.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.26.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.26.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.26.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.26.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.26.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.26.mlp.down_proj.base_layer.weight\n",
      "- model.layers.26.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.26.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.26.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.26.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.26.input_layernorm.weight\n",
      "- model.layers.26.post_attention_layernorm.weight\n",
      "- model.layers.27.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.27.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.27.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.27.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.27.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.27.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.27.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.27.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.27.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.27.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.27.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.27.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.27.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.27.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.27.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.27.mlp.down_proj.base_layer.weight\n",
      "- model.layers.27.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.27.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.27.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.27.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.27.input_layernorm.weight\n",
      "- model.layers.27.post_attention_layernorm.weight\n",
      "- model.layers.28.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.28.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.28.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.28.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.28.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.28.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.28.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.28.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.28.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.28.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.28.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.28.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.28.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.28.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.28.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.28.mlp.down_proj.base_layer.weight\n",
      "- model.layers.28.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.28.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.28.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.28.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.28.input_layernorm.weight\n",
      "- model.layers.28.post_attention_layernorm.weight\n",
      "- model.layers.29.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.29.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.29.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.29.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.29.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.29.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.29.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.29.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.29.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.29.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.29.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.29.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.29.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.29.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.29.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.29.mlp.down_proj.base_layer.weight\n",
      "- model.layers.29.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.29.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.29.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.29.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.29.input_layernorm.weight\n",
      "- model.layers.29.post_attention_layernorm.weight\n",
      "- model.layers.30.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.30.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.30.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.30.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.30.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.30.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.30.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.30.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.30.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.30.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.30.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.30.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.30.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.30.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.30.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.30.mlp.down_proj.base_layer.weight\n",
      "- model.layers.30.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.30.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.30.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.30.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.30.input_layernorm.weight\n",
      "- model.layers.30.post_attention_layernorm.weight\n",
      "- model.layers.31.self_attn.o_proj.base_layer.weight\n",
      "- model.layers.31.self_attn.o_proj.lora_A.vision.weight\n",
      "- model.layers.31.self_attn.o_proj.lora_A.speech.weight\n",
      "- model.layers.31.self_attn.o_proj.lora_B.vision.weight\n",
      "- model.layers.31.self_attn.o_proj.lora_B.speech.weight\n",
      "- model.layers.31.self_attn.qkv_proj.base_layer.weight\n",
      "- model.layers.31.self_attn.qkv_proj.lora_A.vision.weight\n",
      "- model.layers.31.self_attn.qkv_proj.lora_A.speech.weight\n",
      "- model.layers.31.self_attn.qkv_proj.lora_B.vision.weight\n",
      "- model.layers.31.self_attn.qkv_proj.lora_B.speech.weight\n",
      "- model.layers.31.mlp.gate_up_proj.base_layer.weight\n",
      "- model.layers.31.mlp.gate_up_proj.lora_A.vision.weight\n",
      "- model.layers.31.mlp.gate_up_proj.lora_A.speech.weight\n",
      "- model.layers.31.mlp.gate_up_proj.lora_B.vision.weight\n",
      "- model.layers.31.mlp.gate_up_proj.lora_B.speech.weight\n",
      "- model.layers.31.mlp.down_proj.base_layer.weight\n",
      "- model.layers.31.mlp.down_proj.lora_A.vision.weight\n",
      "- model.layers.31.mlp.down_proj.lora_A.speech.weight\n",
      "- model.layers.31.mlp.down_proj.lora_B.vision.weight\n",
      "- model.layers.31.mlp.down_proj.lora_B.speech.weight\n",
      "- model.layers.31.input_layernorm.weight\n",
      "- model.layers.31.post_attention_layernorm.weight\n",
      "- model.norm.weight\n",
      "Components properly unfrozen ✅\n",
      "Gradient accumulation steps: 1\n",
      "Trainable params: 5574460224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 31:50, Epoch 6/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.572700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.002100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configuration variables\n",
    "MODEL_NAME = 'microsoft/Phi-4-multimodal-instruct'\n",
    "OUTPUT_DIR = '/workspace/Phi4_mm_asr_wolbanking77_unf'\n",
    "NEW_MODEL_ID = \"karim155/Phi-4-mm-inst-asr-wolbanking77-unf\"\n",
    "USE_FLASH_ATTENTION = True\n",
    "# BATCH_SIZE_PER_GPU = 8  See dataset loader cell for these parameters.\n",
    "# EVAL_BATCH_SIZE_PER_GPU = 16\n",
    "NUM_TRAIN_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.005\n",
    "\n",
    "# Initialize Accelerator\n",
    "accelerator = Accelerator()\n",
    "num_gpus = accelerator.num_processes\n",
    "print(f\"Training on {num_gpus} GPUs\")\n",
    "\n",
    "def print_model_structure(model, max_depth=3):\n",
    "    \"\"\"Prints model structure up to specified depth\"\"\"\n",
    "    print(\"\\n=== MODEL ARCHITECTURE ===\")\n",
    "    for name, module in model.named_modules():\n",
    "        depth = name.count('.')\n",
    "        if depth < max_depth:\n",
    "            print(f\"{'  ' * depth}{name} ({type(module).__name__})\")\n",
    "\n",
    "def create_model(model_name, use_flash_attention):\n",
    "    \"\"\"Initialize model with audio enabled\"\"\"\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        audio_enabled=True\n",
    "    )\n",
    "    if use_flash_attention:\n",
    "        config._attn_implementation = \"flash_attention_2\"\n",
    "    return AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True\n",
    "    ).to(accelerator.device)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Model Initialization and Unfreezing\n",
    "# --------------------------------------------------\n",
    "with accelerator.local_main_process_first():\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    model = create_model(MODEL_NAME, USE_FLASH_ATTENTION)\n",
    "\n",
    "    def unfreeze_speech_components(model):\n",
    "      \"\"\"Directly target verified components from your debug logs\"\"\"\n",
    "      # 1. Audio Embed Module (confirmed exists)\n",
    "      audio_embed = model.model.embed_tokens_extend.audio_embed\n",
    "\n",
    "      # 2. Entire Audio Encoder (simplified)\n",
    "      audio_encoder = audio_embed.encoder  # Direct access\n",
    "\n",
    "      # 3. Audio Projection (from debug logs)\n",
    "      audio_projection = audio_embed.audio_projection\n",
    "\n",
    "      # Unfreeze ONLY these 3 components\n",
    "      for component in [audio_embed, audio_encoder, audio_projection]:\n",
    "          for param in component.parameters():\n",
    "              param.requires_grad = True\n",
    "      return model\n",
    "\n",
    "    model = unfreeze_speech_components(model)\n",
    "\n",
    "    # Verify unfrozen parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(\"Unfrozen components:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"- {name}\")\n",
    "\n",
    "    # After unfreezing\n",
    "    encoder_params = list(model.model.embed_tokens_extend.audio_embed.encoder.parameters())\n",
    "    proj_params = list(model.model.embed_tokens_extend.audio_embed.audio_projection.parameters())\n",
    "\n",
    "    assert any(p.requires_grad for p in encoder_params), \"Encoder params frozen!\"\n",
    "    assert any(p.requires_grad for p in proj_params), \"Projection params frozen!\"\n",
    "    print(\"Components properly unfrozen ✅\")\n",
    "\n",
    "# Create dataset objects.\n",
    "train_dataset = WolBanking77Dataset(processor, train_ds, training=True)\n",
    "val_dataset = WolBanking77Dataset(processor, val_ds, training=False)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Optimizer Configuration with Correct Gradient Handling\n",
    "# --------------------------------------------------\n",
    "gradient_accumulation_steps = max(1, BATCH_SIZE_PER_GPU // (BATCH_SIZE_PER_GPU // num_gpus))\n",
    "print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "\n",
    "\n",
    "# Set mixed precision flags.\n",
    "fp16 = not USE_FLASH_ATTENTION\n",
    "bf16 = USE_FLASH_ATTENTION\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Training Preparation with DDP Fixes\n",
    "# --------------------------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    ddp_find_unused_parameters=True,  # for unused SigLIP layers\n",
    "    overwrite_output_dir=True,\n",
    "    save_steps=10000,\n",
    "    # num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    max_steps=1000,\n",
    "    per_device_train_batch_size=BATCH_SIZE_PER_GPU,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "    optim='adamw_torch',\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    adam_epsilon=1e-7,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    max_grad_norm=1.0,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=100,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    save_only_model=True,\n",
    "    bf16=bf16,\n",
    "    fp16=fp16,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=2,\n",
    "    push_to_hub=False,\n",
    "    # hub_private_repo=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    hub_model_id=NEW_MODEL_ID\n",
    ")\n",
    "#--------------------------------------------------\n",
    "\n",
    "print(\"Trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Training\n",
    "# --------------------------------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=esb_collate_fn,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save full model with processor and configs\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "# processor.save_pretrained(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c62eb52-4285-45da-8cca-c095d63bf64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers==4.48.2\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | grep transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6149b6f3-8e89-4698-abb7-b5c6232b510c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6188df08b4f9450ebe38f57f3cc55109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating after fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running eval:   0%|          | 0/27 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "running eval: 100%|██████████| 27/27 [01:54<00:00,  4.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 0.03117962930885155\n",
      "WER after fine-tuning: 0.03117962930885155\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = '/workspace/Phi4_mm_asr_wolbanking77_unf'\n",
    "EVAL_BATCH_SIZE_PER_GPU = 24\n",
    "# Free up memory before re-loading the model.\n",
    "# del model, trainer\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reload the fine-tuned model.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    OUTPUT_DIR,\n",
    "    trust_remote_code=True,\n",
    "    # torch_dtype='auto',\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    _attn_implementation='flash_attention_2',\n",
    ").cuda()\n",
    "model = torch.compile(model)\n",
    "model.eval()  # Ensure evaluation mode.\n",
    "\n",
    "# Evaluate the model after fine-tuning.\n",
    "print(\"Evaluating after fine-tuning...\")\n",
    "wer_after = evaluate(\n",
    "    model,\n",
    "    processor,\n",
    "    val_dataset,\n",
    "    save_path=Path(training_args.output_dir) / 'eval_after.json',\n",
    "    eval_batch_size=EVAL_BATCH_SIZE_PER_GPU,\n",
    ")\n",
    "print(f\"WER after fine-tuning: {wer_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c06c669-e3e0-4a92-8a48-b2429248a143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating after fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running eval:   0%|          | 0/27 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "running eval: 100%|██████████| 27/27 [01:55<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 0.03117962930885155\n",
      "WER after fine-tuning: 0.03117962930885155\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = '/workspace/Phi4_mm_asr_wolbanking77_unf'\n",
    "MODEL_NAME = 'microsoft/Phi-4-multimodal-instruct'\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "val_dataset = WolBanking77Dataset(processor, val_ds, training=False)\n",
    "# Evaluate the model after fine-tuning.\n",
    "print(\"Evaluating after fine-tuning...\")\n",
    "wer_after = evaluate(\n",
    "    model,\n",
    "    processor,\n",
    "    val_dataset,\n",
    "    save_path=Path(OUTPUT_DIR) / 'eval_after.json',\n",
    "    eval_batch_size=EVAL_BATCH_SIZE_PER_GPU,\n",
    ")\n",
    "print(f\"WER after fine-tuning: {wer_after}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1719c5c-d041-4c9d-a7f1-b5a94b6b0a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4570b6-2396-432c-a701-8f879864e3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
